app.py
Minimal Streamlit app to verify PDF uploads (lecture notes + reference books).
- Shows filenames and sizes
- Enforces PDF type and warns if >10 reference PDFs
import streamlit as st

st.set_page_config(page_title="Lecture Notes Assistant (Upload Test)", page_icon="üìÑ", layout="centered")
st.title("üìÑ Lecture Notes Assistant ‚Äî Upload Test")

st.markdown(
"Use this minimal app to confirm uploads work. "
"It only lists filenames and sizes‚Äîno processing yet."
)

--- Upload widgets ---
notes_file = st.file_uploader(
"Upload Lecture Notes PDF (single file)",
type=["pdf"],
accept_multiple_files=False,
help="Handwritten/scanned notes are fine. PDF only."
)

refs_files = st.file_uploader(
"Upload Reference Books PDFs (up to 10)",
type=["pdf"],
accept_multiple_files=True,
help="Select multiple PDFs; this app will just echo their names & sizes."
)

--- Echo uploads ---
st.subheader("Upload Summary")

if notes_file is not None:
size_bytes = getattr(notes_file, "size", None)
size_str = f"{size_bytes/1024:.1f} KB" if size_bytes is not None else "unknown size"
st.success(f"Lecture Notes: {notes_file.name} ¬∑ {size_str}")
else:
st.info("No lecture notes uploaded yet.")

if refs_files:
if len(refs_files) > 10:
st.warning(f"You selected {len(refs_files)} reference PDFs. Only the first 10 are permitted.")
refs_files = refs_files[:10]

st.write(f"**Reference PDFs:** {len(refs_files)} file(s)")
for i, f in enumerate(refs_files, start=1):
    size_bytes = getattr(f, "size", None)
    size_str = f"{size_bytes/1024:.1f} KB" if size_bytes is not None else "unknown size"
    st.write(f"- {i}. **{f.name}**  ¬∑  {size_str}")
else:
st.info("No reference PDFs uploaded yet.")

--- Optional "Generate Report" button (no-op for now) ---
if st.button("Generate Report (No-Op)"):
if not notes_file:
st.error("Please upload a Lecture Notes PDF.")
elif not refs_files:
st.error("Please upload at least one Reference Book PDF.")
else:
st.success("Uploads look good ‚úÖ (This button is a no-op in the upload test app.)")

--- Footer hint ---
st.caption("Tip: run with streamlit run app.py. This is only an upload sanity check.")


I‚Äôve read through the Streamlit upload test you shared. What would you like me to help with‚Äîadd features, review the code, or something else?

You are writing a single Python module named utils/embedding_indexer.py.

Goal:
Implement a chunking & embedding/indexing helper used by a Streamlit app. It must:

Split long text into chunks.
‚Ä¢ OPTION A (REQUIRED): For character-based chunking, DO NOT re-implement splitting here.
Instead, CALL utils.pdf_utils.split_text(pages=..., ...) and flatten the results to a list of chunks.
‚Ä¢ Provide an OPTIONAL token-based path using tiktoken if available.
Batch-embed chunks using our existing utils.embeddings helpers (no direct OpenAI calls here).
Upsert vectors (with metadata) into a Pinecone index using our utils.pinecone_compat.
Provide a cached index initializer.
Assumptions / existing utilities:

utils.pdf_utils exposes:
split_text(pages: List[str], mode: str = "auto", max_tokens: int = 480, min_chars: int = 200, overlap_tokens: int = 40, tokenizer: Optional[Callable] = None, join_short: bool = True) -> Tuple[List[str], List[Dict]]
Returns (segments, spans). For this module, use only the segments list when delegating char-based splitting.
You will pass in a single-element list [text] as pages for whole-text splitting, unless the caller gives you per-page texts.
utils.embeddings exposes:
EMBED_DIM: int (e.g., 3072 for text-embedding-3-large)
get_embedding(text: str) -> List[float]
batch_get_embeddings(texts: List[str]) -> List[List[float]]
utils.pinecone_compat exposes:
get_client() -> Any
ensure_index(client: Any, name: str, dim: int, metric: str = "cosine") -> Any
upsert(index: Any, vectors: Sequence[Tuple[str, List[float], Dict]]) -> None
File requirements:

Use typing annotations and clear docstrings.
No hard-coded API keys; this file only orchestrates helpers.
Be robust if tiktoken is not installed (fallback to delegating to pdf_utils.split_text).
Add small, readable logging via print statements for batch progress (OK for prototype).
Raise sensible errors on bad input lengths (e.g., chunks vs metadata mismatch).
IMPORTANT: Do not duplicate character-based splitting logic in this file. Reuse pdf_utils.split_text for char/paragraph/sentence modes.
Public functions to implement:

def chunk_text(      text: str,      *,      # Char/paragraph/sentence path (delegates to pdf_utils.split_text)      char_mode: str = "auto",            # "auto" | "paragraphs" | "sentences" | "tokens"      min_chars: int = 200,      # Token path (only when char_mode == "tokens")      chunk_size_tokens: int = 500,      overlap_tokens: int = 50,      tokenizer_name: str = "cl100k_base",  ) -> List[str]:

If char_mode != "tokens":
Delegate to utils.pdf_utils.split_text(pages=[text], mode=char_mode, max_tokens=480, min_chars=min_chars, overlap_tokens=overlap_tokens, tokenizer=None, join_short=True)
and return the resulting segments.
Ensure the returned list contains no empty strings (filter if needed).
Else (token mode):
Try to import tiktoken. If available, tokenize with tiktoken.get_encoding(tokenizer_name).
Build sliding windows of chunk_size_tokens with overlap_tokens overlap.
Normalize whitespace lightly (collapse >2 newlines to 2, strip ends), but do not lowercase/remove symbols.
If tiktoken import fails, FALL BACK to the delegate path above (split_text with mode="auto").
If input text is None or empty, return [].
def embed_chunks( chunks: List[str], *, batch_size: int = 64, ) -> List[List[float]]:

Use batch_get_embeddings from utils.embeddings in batches of batch_size.
Return a list of vectors aligned to chunks.
If chunks is empty, return [].
def init_pinecone_index( index_name: str, embedding_dim: int = EMBED_DIM, metric: str = "cosine", ) -> Any:

Cache and return the Pinecone index handle for reuse.
Use get_client() and ensure_index(...).
Subsequent calls with the same index_name should reuse the cached handle.
Maintain module-level globals: _cached_index_handle, _cached_index_name, _client.
def index_chunks( chunks: List[str], metadata_list: List[dict], *, index_name: str, id_prefix: str = "chunk", batch_size: int = 100, ) -> None:

Validate lengths: len(chunks) == len(metadata_list).
Call embed_chunks(chunks) to get vectors.
Construct vector tuples (id: str, values: List[float], metadata: Dict).
IDs may be f"{id_prefix}-{i}".
Ensure metadata contains "text": chunk if not already present.
Upsert in batches using utils.pinecone_compat.upsert.
Print simple progress like: Upserting batch X/Y ....
Also include:

A small _normalize_text helper if needed for the token-based path (but DO NOT reimplement char-based split logic).
Module-level cached globals: _cached_index_handle, _cached_index_name, _client.
Edge cases:

If text is None or empty, chunk_text returns [].
For token mode, overlap must be < chunk size. If not, reduce overlap to chunk_size_tokens - 1.
If tiktoken import fails or raises, fall back to pdf_utils.split_text with mode="auto".
At the bottom, include a __main__ block that:

Demonstrates char-based delegation on a sample string:
segments = chunk_text("Para1...\n\nPara2...\n", char_mode="auto")
Does NOT attempt any real Pinecone or OpenAI calls (just prints chunk stats).
Example printout: number of chunks and first 100 chars of the first chunk.
Constraints:

Keep the implementation concise and readable.
Absolutely avoid duplicating split_text logic. Always call into utils.pdf_utils.split_text for non-token paths.
Now write the complete utils/embedding_indexer.py code.


Implemented utils/embedding_indexer.py with end-to-end chunking, embedding, and Pinecone indexing helpers. Key points:

chunk_text delegates non-token splits to utils.pdf_utils.split_text and adds a tiktoken-backed token path with normalization and overlap guards (utils/embedding_indexer.py (line 33)).
embed_chunks batches calls to batch_get_embeddings with progress prints and alignment checks (utils/embedding_indexer.py (line 85)).
init_pinecone_index now caches the client/index handle for reuse, while index_chunks validates metadata, injects missing text fields, embeds, and upserts in batches with logging (utils/embedding_indexer.py (line 112), utils/embedding_indexer.py (line 132)).
Added a lightweight __main__ demo that showcases delegated chunking without hitting external services (utils/embedding_indexer.py (line 163)).
Next steps you might consider:

Wire these helpers into the Streamlit flow and confirm chunk/metadata generation matches expectations.
Add unit tests for chunking fallbacks and Pinecone batching (mocking the external helpers).
You are writing a single Python module named planner/planner_agent.py.

Goal:
Implement a hybrid Planner agent that decides, per lecture-segment, whether to (a) search as-is, (b) refine and retry, or (c) declare ‚Äúexternal‚Äù and suggest a book via LLM. Use heuristic thresholds first; escalate to LLM only if needed. Keep it thin and production-friendly for a 2-day prototype.

Key requirement (LLM toggle):

Make the LLM provider pluggable with a simple toggle.
Default provider: OpenAI.
Optional provider: Anthropic (Claude).
The code must be very easy to edit to add/replace providers:
Implement provider adapters: _call_llm_planner_openai(...), _call_llm_planner_anthropic(...).
Select provider via PlannerConfig.provider (defaults to "openai"). Allow PLANNER_PROVIDER env to override.
Auto-disable LLM if API key or provider package is unavailable (fail-open to heuristics).
Keep all provider-specific imports inside their adapter functions (import-guarded) to avoid hard dependencies.
Assumptions / existing utilities:

Segmenting (reuse, do not re-implement splitting here):
utils.pdf_utils.split_text(pages: List[str], mode="auto", max_tokens=480, min_chars=200, overlap_tokens=40, tokenizer=None, join_short=True) -> Tuple[List[str], List[Dict]]
Retriever worker:
workers.retriever.RetrieverWorker with:
init(self, index_name: str, top_k: int = 5)
search(self, query_text: str, top_k: int | None = None) -> List[Dict]
Each match: {"text": str, "metadata": dict, "score": float}
(If the import fails, provide a light fallback stub that raises NotImplementedError.)
Compiler worker will consume our result dicts later; we only assemble fields it needs.
No local model hosting; LLM calls via provider APIs if available.
File requirements:

Use typing and dataclasses.
Small, readable logging to logs/planner_decisions.log (append mode). Create dir if missing.
Keep the per-segment UI plan trace ‚â§ 10 words (e.g., "Normal search", "Refine & retry", "Lowered œÑ", "External book suggested", "Stop: budget reached").
Heuristics before LLM:
œÑ_accept (high-confidence): accept immediately.
œÑ_refine (gray zone): attempt 1‚Äì2 refine tries before giving up.
œÑ_fail (low-confidence): skip to LLM plan immediately.
adaptive lowering: optionally lower œÑ_refine by delta once per segment.
max_steps: hard stop per segment (e.g., 3).
Public API:

@dataclass class PlannerConfig:
index_name: str
top_k: int = 5
tau_accept: float = 0.83
tau_refine: float = 0.68
tau_fail: float = 0.45
adaptive_delta: float = 0.05 # one-time lowering of tau_refine
max_refines: int = 1 # refine attempts per segment
max_steps: int = 3 # total decisions per segment

LLM toggle
llm_enabled: bool = True
provider: str = "openai" # "openai" (default) | "anthropic"
model: str | None = None # provider-specific default if None
prompt_path: str = "prompts/planner_prompt.txt"

def plan_segment(segment_text: str, retriever: "RetrieverWorker", cfg: PlannerConfig) -> dict:
Behavior:

Step 0: quick exit if blank/very short ‚Üí NORMAL.
Step 1: Heuristic search with original text:
q = segment_text
r = retriever.search(q, top_k=cfg.top_k)
score = (r[0]["score"] if r else 0.0)
If score >= tau_accept ‚Üí decision="NORMAL", query_text=q, trace="Normal search"
If score <= tau_fail ‚Üí escalate_to_llm()
Else (tau_fail < score < tau_accept):
Try refine loop up to cfg.max_refines:
If cfg.llm_enabled and provider usable ‚Üí call LLM planner
- If REFINE:<q2> ‚Üí rerun search(q2)
- If best >= tau_refine ‚Üí decision="REFINE", query_text=q2, trace="Refine & retry"
- Else if adaptive not yet used: lower tau_refine by adaptive_delta and accept if now >= adjusted
- Else continue / exhaust
- If NORMAL ‚Üí accept current top ‚Üí decision="NORMAL", trace="LLM: normal"
- If EXTERNAL:<book> ‚Üí decision="EXTERNAL", suggested_book=book, trace="External book suggested"
Else (no LLM): lower tau_refine once and re-check; if still low, fall back to NORMAL
If refinements exhausted and still low: decision="NORMAL" (fail-open), trace="Stop: budget reached"
Writes one compact line to logs/planner_decisions.log (segment hash, decision, trace, best_score).
Returns dict:
{
"segment_text": str,
"decision": "NORMAL"|"REFINE"|"EXTERNAL",
"query_text": str | None,
"suggested_book": str | None,
"trace": str, # ‚â§ 10 words
"best_score": float | None
}
def process_lecture(lecture_text: str, cfg: PlannerConfig) -> list[dict]:

Split into segments with utils.pdf_utils.split_text(pages=[lecture_text], mode="auto", ...).
Create retriever = RetrieverWorker(cfg.index_name, top_k=cfg.top_k).
For each segment, call plan_segment(...) and collect results.
Return list of segment result dicts.
LLM planner prompt:

Load from cfg.prompt_path if exists; else use a safe inline default:
System: "You are a planning assistant for academic retrieval decisions."
User:
Segment:
"""{segment_text}"""
Decide and output EXACTLY one of:
NORMAL
REFINE: <rewritten query>
EXTERNAL: <textbook title>
Provider toggle & adapters (VERY IMPORTANT):

Resolve provider at runtime as:
provider = (cfg.provider or os.environ.get("PLANNER_PROVIDER") or "openai").lower()

Determine usability:
OPENAI requires env OPENAI_API_KEY; Anthropic requires ANTHROPIC_API_KEY.
If missing or import fails, set usable=False and skip LLM (heuristics only).

Implement adapter functions (imports inside functions):

def _call_llm_planner_openai(system_prompt: str, user_prompt: str, model: str | None) -> str:
# import guarded
try:
from openai import OpenAI
except Exception:
raise RuntimeError("OpenAI provider not available")
m = model or "gpt-4o-mini"
client = OpenAI()
resp = client.chat.completions.create(
model=m,
messages=[{"role":"system","content":system_prompt},
{"role":"user","content":user_prompt}],
temperature=0
)
return resp.choices[0].message.content.strip()

def _call_llm_planner_anthropic(system_prompt: str, user_prompt: str, model: str | None) -> str:
# import guarded
try:
import anthropic
except Exception:
raise RuntimeError("Anthropic provider not available")
m = model or "claude-3-5-sonnet-latest"
client = anthropic.Anthropic()
resp = client.messages.create(
model=m,
system=system_prompt,
messages=[{"role":"user","content":user_prompt}],
max_tokens=256,
temperature=0
)
return resp.content[0].text.strip()

Provider registry & selector:

PROVIDERS = {
"openai": _call_llm_planner_openai,
"anthropic": _call_llm_planner_anthropic,
}

def _llm_available(provider: str) -> bool:
if provider == "openai": return bool(os.getenv("OPENAI_API_KEY"))
if provider == "anthropic": return bool(os.getenv("ANTHROPIC_API_KEY"))
return False

def _call_llm_planner(provider: str, system_prompt: str, user_prompt: str, model: str | None) -> str:
func = PROVIDERS.get(provider)
if not func:
raise RuntimeError(f"Unknown provider: {provider}")
return func(system_prompt, user_prompt, model)

Parsing the LLM output:
Trim, uppercase prefix; accept:
"NORMAL"
"REFINE: <...>"
"EXTERNAL: <...>"
On anything else ‚Üí treat as NORMAL (fail-open).

Logging:

Ensure logs/ exists; append lines like:
"[seg=ab12c] decision=NORMAL trace='Normal search' score=0.87"
seg hash = first 5 hex chars of sha1(segment_text).
Robustness:

Wrap retriever & LLM calls with try/except; on exceptions, default to NORMAL and continue.
Keep the module free of Streamlit dependencies.
Also include:

Helper functions: _sha1_short, _load_prompt, _write_log_line
Minimal fallback stub for RetrieverWorker (if import fails, raise NotImplementedError in search()).
A __main__ block that runs a quick dry test with LLM disabled (llm_enabled=False), printing a couple of sample decisions & traces.
Now write the complete planner/planner_agent.py code implementing everything above.


Implemented a heuristics-first planner agent with optional LLM escalation for lecture segments.

Added an import-guarded RetrieverWorker stub so the module loads cleanly when the worker package is absent (planner/planner_agent.py (line 17)).
Expanded PlannerConfig with threshold, refinement, and provider toggles, honoring env overrides for fast edits (planner/planner_agent.py (line 33)).
Introduced lightweight logging and prompt-loading helpers to keep decisions auditable without extra dependencies (planner/planner_agent.py (line 56), planner/planner_agent.py (line 65)).
Registered pluggable OpenAI/Anthropic adapters with availability checks and a single dispatcher for planner prompts (planner/planner_agent.py (line 109), planner/planner_agent.py (line 146)).
Implemented plan_segment with tiered heuristics, adaptive thresholds, LLM-guided refinements, and consistent logging outcomes (planner/planner_agent.py (line 195)).
Wired lecture processing through the existing splitter and added a no-LLM demo harness for quick smoke tests (planner/planner_agent.py (line 399), planner/planner_agent.py (line 423)).
Tests: python -m compileall planner/planner_agent.py

Next steps you might consider:

Add unit tests around plan_segment to cover threshold edge cases and LLM fallbacks.
Run an end-to-end check with the real RetrieverWorker to tune the default œÑ values against live scores.
In embedding_indexer.py, the chunk_text function contains an outdated call to pdf_utils.split_text inside the if char_mode != "tokens": block. This call uses incorrect parameters (like pages, max_tokens, mode) and incorrectly destructures the return value as a tuple.

Please replace the contents of the if char_mode != "tokens": block in embedding_indexer.py to correctly call the pdf_utils.split_text function.

The correct call should:

Pass the text variable as the text argument.

Pass the min_chars variable as the min_chunk_len argument.

Pass the overlap_tokens variable as the overlap argument (we will assume the variable name is just misleading and it holds a character value).

Pass the hard-coded 480 (from the old max_tokens argument) as the max_length argument.

Assign the direct list returned by the function to the segments variable (e.g., segments = pdf_utils.split_text(...)).

Return the cleaned list of segments.


Updated the char-mode path in utils/embedding_indexer.py (line 38) to call the new pdf_utils.split_text signature directly (text, min_chunk_len, overlap, max_length) and to use the list it returns, keeping the filtered segment output unchanged. Nothing else touched.

you've done the same error in planner_agent.py too. can you refer to the actual split_text() function from the pdf_utils.py and correct this mistake accordingly?


Updated planner/planner_agent.py (line 403) to call the new pdf_utils.split_text signature correctly‚Äîpassing text, max_length, overlap, and min_chunk_len, and using the returned list directly.

You are writing a single Python module named workers/retriever.py.

Goal:
Implement a thin RetrieverWorker used by the Planner and Compiler. It must:

Call our vector store to retrieve top matches for a query.
Normalize/format matches into uniform excerpts with rich citation fields.
Support a per-instance min_score threshold, dedup, and simple stitching.
Provide helper to format a short ‚Äútop snippets‚Äù glimpse string for LLM prompts.
Work with the existing Planner which calls retriever.search(query_text, top_k=None).
Existing repo APIs to use (do not re-implement):

vector_store.search_vector_store(query_text: str, *, index_name: str, top_k: int = 5) -> List[Dict]]
Returns items shaped like: {"id": str, "text": str|None, "metadata": dict, "score": float}
Design:

A class RetrieverWorker with:
__init__(self, index_name: str, top_k: int = 5, min_score: float = 0.30, dedup: bool = True)
search(self, query_text: str, top_k: int | None = None) -> List[dict] # used by Planner
retrieve_excerpts(self, query_text: str, top_k: int = 3) -> List[dict]
retrieve_for_segments(self, segment_plans: List[dict], *, attempt_on_external: bool = False, top_k: int = 3) -> List[dict]
format_top_snippets(self, matches: List[dict], limit: int = 3) -> str
Keep the module import-only (no Streamlit, no global state).
Small, readable prints for debugging are OK, but keep output minimal.
Normalization & citation fields:

Each excerpt dict should be:
{
"text": str, # snippet text
"book": str, # from metadata['book'] or ['title'] (fallback "Unknown Book")
"page": int | str | None, # metadata page (page/page_number/pages)
"pages": list | None, # if a list/range is available
"figure_nums": list | None, # if present in metadata (e.g., ["3.2", "5.4"])
"chunk_id": str | None, # original match id if present
"score": float, # similarity score, higher is better
"metadata": dict # original metadata passthrough
}
Robustly extract:
book := meta.get("book") or meta.get("title") or "Unknown Book"
pages := meta.get("pages") if list-like else wrap single page into list
page := if meta has single page key: meta.get("page") or meta.get("page_number")
figure_nums := meta.get("figures") or meta.get("figure") or meta.get("figure_nums")
Normalize score to float; if missing/invalid, treat as 0.0.
Filtering, sorting, dedup:

Sort matches by score descending.
Drop items with score < self.min_score.
Deduplicate on normalized snippet text (strip and collapse whitespace); keep first occurrence.
Optional stitching (very light):

If two consecutive kept excerpts come from the same book and adjacent page(s), optionally join them into one longer excerpt (keep the higher score and union page info). Keep this simple and safe.
Top snippet glimpse:

format_top_snippets(matches, limit=3) ‚Üí returns a short multi-line string:
"- score=0.72 | title=Shapiro | snippet=<first 140 chars>"
Use this to feed Planner prompts (kept under 3 lines, no trailing spaces).
Behavior for EXTERNAL plans:

retrieve_for_segments(..., attempt_on_external=False):
If plan["decision"] == "EXTERNAL" and attempt_on_external is False: set plan["excerpts"] = [], plan["no_match"] = True, add plan["top_snippets"] = "" and continue.
If attempt_on_external is True, still try retrieval.
Implementation details:

Use only: from vector_store import search_vector_store.
Be strict about index_name coming from the instance.
Keep signatures exactly as specified above.
Add docstrings and type hints.
Also include:

A small _norm_text(s: str) -> str and _extract_fields(meta: dict) -> tuple helpers.
A __main__ block that runs a dry test ONLY if a dummy fallback is provided (i.e., do not require Pinecone/OpenAI). If the import works but no env is set, just print a short message and exit gracefully.
Now write the complete workers/retriever.py code.

Example behavior (for reference; do not print this in code):

search("area‚ÄìMach relation") ‚Üí returns raw matches (text/metadata/score)
retrieve_excerpts("area‚ÄìMach relation") ‚Üí returns cleaned excerpts with book/page/score
format_top_snippets(matches) ‚Üí returns 1‚Äì3 lines for prompt context
retrieve_for_segments(plans) ‚Üí fills each plan with excerpts and top_snippets

Created workers/retriever.py with a production-ready RetrieverWorker that wraps vector_store.search_vector_store and normalizes results. Highlights:

Added normalization helpers for whitespace and safe score parsing, plus robust metadata extraction for book/page/figure info (workers/retriever.py (line 12), workers/retriever.py (line 17), workers/retriever.py (line 25)).
Implemented RetrieverWorker methods for search, excerpt retrieval, segment enrichment, and snippet formatting, honoring min-score filtering, deduplication, and LLM prompt needs (workers/retriever.py (line 60)).
Added lightweight stitching of adjacent excerpts from the same book to merge consecutive pages without data loss (workers/retriever.py (line 154)).
Included an env-guarded __main__ dry run for quick diagnostics (workers/retriever.py (line 208)).
Next step: run an integration test with your vector store index to ensure snippet stitching and thresholds feel right for real content.

You are writing a single Python module named workers/compiler.py.

Goal:
Implement a thin CompilerWorker that turns segment plans (with retrieved excerpts) into a clean PDF:

Consistent citation formatting (book + page or page range).
Clear structure: segment text, matched excerpts (with citations), notes for no-match/external.
"Figure Pointers" appendix listing figure IDs with book + page(s).
Return both: (a) write-to-path and (b) return-bytes variants for Streamlit.
Assumptions / input shape (matches our repo):

Each segment plan dict (from Planner + Retriever) may contain:
{
"segment_text": str,
"decision": "NORMAL" | "REFINE" | "EXTERNAL",
"query_text": str | None,
"suggested_book": str | None,
"trace": str, # <= 10 words
"best_score": float | None,
"excerpts": [ # from RetrieverWorker
{
"text": str,
"book": str,
"page": int|str|None,
"pages": list|None,
"figure_nums": list|None,
"chunk_id": str|None,
"score": float,
"metadata": dict,
}, ...
],
"no_match": bool, # optional
"top_snippets": str # optional, short multi-line glimpse
}
Requirements:

Use fpdf2 (from fpdf import FPDF) ‚Äì no other heavy deps.
Handle long paragraphs via multi_cell.
Provide a small PDF header/footer (title + page numbers).
Be robust to Unicode: replace unsupported characters gracefully (latin-1 fallback).
Keep layout readable and consistent (margins, spacing).
Do NOT embed images; only list ‚ÄúFigure Pointers‚Äù (figure ID + book + page).
Keep functions small and documented.
Public API to implement:

def format_citation(book: str, page: int | str | None, pages: list | None = None) -> str:

Format like: "(Source: {book}, p.{page})"
If pages has multiple entries, show a compact range or comma-joined:
single: p.12
list: pp. 12‚Äì14 (when consecutive), else "pp. 12, 14, 17"
If no page info, show "(Source: {book})".
def compile_report(segment_plans: list[dict], output_pdf_path: str, *,
title: str = "Lecture Notes‚ÄîMatched Excerpts",
author: str = "Lecture Notes Assistant",
include_plan_table: bool = True) -> None:

Build and save the PDF to output_pdf_path.
Structure:
A) Title page with title + run date/time.
B) (Optional) ‚ÄúPlanner Decisions‚Äù mini-table listing:
Segment # | Decision | Trace | Best Score
(Use fixed-width or simple multi_cell lines.)
C) Per-segment section:
- Heading: "Segment {i}"
- Segment text in italics (or prefixed with ‚ÄúLecture segment:‚Äù)
- If excerpts present:
‚Ä¢ For each excerpt: a bullet-style block with the excerpt text (trim if extremely long),
followed by the citation from format_citation.
Also show "(score=0.78)" on a new light line if available.
‚Ä¢ Accumulate figure references while scanning excerpt text via regex
r'(?i)\bfig(?:ure)?.?\s*(\d+(?:.\d+)*)'
For each match, record (figure_id, book, page or pages) in a set.
- If no excerpts:
‚Ä¢ If decision == "EXTERNAL" and suggested_book present:
show: "[No excerpt found in provided references; suggested external source: {suggested_book}]"
‚Ä¢ Else:
show: "[No relevant excerpt found in provided references]"
- Add a little spacing between segments.
D) Appendix page: "Figure Pointers"
- If any collected figures:
‚Ä¢ Deduplicate and sort by (book, figure_id)
‚Ä¢ Output lines like: "Figure 3.2 ‚Äî see {book}, p.10" or "pp. 10‚Äì11"
Else:
‚Ä¢ Print "(No figure references found in excerpts)"
Add page numbers in footer ("Page {n}") and set PDF metadata (title, author).
def compile_report_bytes(segment_plans: list[dict], *,
title: str = "Lecture Notes‚ÄîMatched Excerpts",
author: str = "Lecture Notes Assistant",
include_plan_table: bool = True) -> bytes:

Same as compile_report but returns PDF as bytes (use io.BytesIO).
Useful for Streamlit‚Äôs st.download_button.
Implementation details:

Define a tiny subclass class PDFReport(FPDF): with:
header(): sets font, prints centered title on first line (small top margin)
footer(): page number centered at bottom
Set auto page break with margin (e.g., 15).
Fonts: use built-in "Times" or "Helvetica".
Line heights: ~6‚Äì8 for 12pt font; consistent spacing.
Text safety:
Implement _safe(text: str) -> str that encodes to latin-1 with 'replace'
then decodes back, so FPDF never crashes on Unicode math characters.
Use _safe for any user/excerpt text passed to FPDF.
Trimming:
If an excerpt is extremely long (e.g., > 4000 chars), truncate and append " ‚Ä¶".
Utilities:
_flatten_pages(page, pages) -> (display_page, display_pages_list) to harmonize usage in citations & figure pointers.
_compact_page_list(pages: list) -> str to format ranges (consecutive as "12‚Äì14", otherwise "12, 14, 17").
_gather_fig_refs(excerpt_text, book, page, pages, into_set) to collect figure refs.
_write_plan_table(pdf, segment_plans) for the optional decisions table.
Also include:

A minimal __main__ that creates a tiny mock segment_plans with 2 segments
(one with excerpts, one EXTERNAL), then writes compiler_demo.pdf to cwd
and prints "OK" without requiring any external services.
Now write the complete workers/compiler.py code implementing everything above.


Implemented workers/compiler.py to generate polished PDF reports from planner/retriever output.

Added safe text handling, page/citation utilities, figure reference gathering, and a custom PDFReport subclass with headers/footers.
format_citation now normalizes single pages and ranges, outputting consistent ‚ÄúSource‚Äù strings.
Built report assembly pipeline (title page, optional planner table, per-segment sections, figure appendix), with compile_report (path) and compile_report_bytes (bytes) variants sharing a common builder.
Ensures excerpts are trimmed, citations shown, scores noted, external/no-match segments flagged, and figure pointers summarized as ‚ÄúFigure X ‚Äî see ‚Ä¶‚Äù.
Included __main__ demo that writes compiler_demo.pdf for quick sanity checks.
Try running python workers/compiler.py to produce the demo PDF and confirm layout.

You are updating an existing module workers/compiler.py.

Context:
This module already implements:

format_citation(book, page, pages=None)
compile_report(segment_plans, output_pdf_path, *, title, author, include_plan_table)
compile_report_bytes(segment_plans, *, title, author, include_plan_table)
A PDFReport(FPDF) subclass with header/footer
Helpers for safe text, figure extraction, and a simple decisions table
Task:
Apply the following surgical enhancements WITHOUT breaking the public API, while keeping code clear and documented.

Language & typing:

Add at the top: from __future__ import annotations (Python 3.10+ unions: int | str | None)
Ensure all public functions and key helpers have type hints and docstrings.
Excerpts cap
Add a new kwarg to both compile_report and compile_report_bytes:
max_excerpts_per_segment: int = 3
When rendering a segment‚Äôs excerpts:
Sort by score descending (if present).
Render only the top max_excerpts_per_segment.
If more excerpts exist, append a final note line:
"(+N more hidden)" where N is the number not displayed.
Score display formatting
When an excerpt has a score, show it on its own light/gray line:
"(score=0.78)" rounded to 2 decimals.
Implement via temporary pdf.set_text_color(120,120,120) before writing the score line, then reset to black (0,0,0).
Page normalization & explicit behavior
Implement _normalize_page_fields(page: int | str | None, pages: list | None) -> tuple[int | None, list[int]]
Behavior:
Coerce to integers where possible ("10" -> 10).
For pages, drop non-numeric entries, de-duplicate, and sort.
Return: (single_page_int_or_None, normalized_pages_list)
Update format_citation(...) to call _normalize_page_fields(...).
If pages has consecutive ranges, display compactly:
single page: p. 12
consecutive: pp. 12‚Äì14
non-consecutive: pp. 12, 14, 17
If both page and pages exist, prefer pages when it‚Äôs non-empty; otherwise use page.
If neither exists: (Source: {book}).
Figure regex robustness
Update the figure extraction regex to capture IDs in:
‚ÄúFig. 3.2‚Äù, ‚ÄúFigure 3.2‚Äù, ‚ÄúFig 3.2(a)‚Äù
Use (case-insensitive):
r'(?i)\bfig(?:ure)?\.?\s*(\d+(?:\.\d+)*)(?:[a-z])?'
In _gather_fig_refs(...), store only the captured numeric part (e.g., "3.2") as figure_id (discard trailing letters).
Page breaks & headings
Before writing each ‚ÄúSegment {i}‚Äù heading, if remaining space is small, add a page break:
If pdf.get_y() > pdf.h - 40, call pdf.add_page().
Start each segment with a bold heading (e.g., ‚ÄúSegment {i}‚Äù) and then the lecture segment text (e.g., italic or prefixed).
Safe fallbacks when fonts missing
Wrap set_font(...) calls in a small helper _set_font(pdf, family="Times", style="", size=12):
try: pdf.set_font(family, style=style, size=size)
except Exception: pdf.set_font("Helvetica", style="", size=size)
Use _set_font everywhere instead of direct set_font.
Deterministic appendix ordering
When rendering the ‚ÄúFigure Pointers‚Äù appendix, sort entries deterministically:
Sort key: (book.lower(), natural_sort_key(figure_id))
Implement natural_sort_key(s: str) -> list that splits runs of digits into integers and other chars into lowercase strings, e.g.:
"10.2" -> [10, ".", 2], "3.10" -> [3, ".", 10].
Unicode safety
Keep _safe(text: str) -> str helper that encodes to latin-1 with 'replace' and decodes back.
Use _safe(...) for all text passed to FPDF (segment text, excerpts, citations, notes).
Appendix content
Continue deduplicating (figure_id, book, page/pages) entries using a set.
When printing page info for a figure pointer, reuse the same page formatting as in format_citation (single p. vs pp. with ranges).
Unit sanity check (at bottom, guarded)
Add a small if __name__ == "__main__": block that:
Builds a segment_plans sample with:
Segment 1 having 4 excerpts from book ‚ÄúX‚Äù, with pages: page="10", pages=["10","11","NaN"], and text containing ‚ÄúFig. 3.2(a)‚Äù.
Segment 2 with decision="EXTERNAL" and suggested_book="Attention Is All You Need".
Asserts:
format_citation("X", page=None, pages=[10,11]) == "(Source: X, pp. 10‚Äì11)"
Calls compile_report(segment_plans, "compiler_demo.pdf", max_excerpts_per_segment=3) and compile_report_bytes(...).
Prints the byte size returned from compile_report_bytes(...) and "OK".
Acceptance criteria:

The module keeps the same public API and returns identical results for unchanged inputs.
New kwargs are optional and default to current behavior (3 excerpts cap).
New page normalization and figure regex work with mixed strings/ints.
The appendix is deterministically ordered and shows compact page ranges.
Fonts never crash the run (Helvetica fallback).
The unit check at the bottom runs without external services and prints sizes.
Now apply these updates to workers/compiler.py. Do not remove existing helpful comments/docstrings; extend them where needed.


Implemented the requested compiler upgrades: excerpts are normalized, sorted, capped, and cited consistently.

Added shared helpers for page coercion, compact ranges, natural figure sorting, and resilient font handling so citations and appendix reuse the same logic (workers/compiler.py (line 27), workers/compiler.py (line 53), workers/compiler.py (line 80), workers/compiler.py (line 115), workers/compiler.py (line 129)).
Updated segment rendering to honor max_excerpts_per_segment, display scores in gray, record hidden counts, and gather figure pointers with the new regex (workers/compiler.py (line 175), workers/compiler.py (line 198), workers/compiler.py (line 212), workers/compiler.py (line 220)).
Refined citation + appendix formatting to use normalized page data and deterministic ordering, sharing the _format_page_fragment helper (workers/compiler.py (line 91), workers/compiler.py (line 245), workers/compiler.py (line 293)).
Expanded the report builders to thread the excerpt cap through both file/bytes outputs, keeping the API backward compatible with a default of three excerpts (workers/compiler.py (line 320), workers/compiler.py (line 355), workers/compiler.py (line 375)).
Added the self-test harness as requested so the module can demo PDF creation and assert citation behavior (workers/compiler.py (line 394)).
Tests: python -m compileall workers/compiler.py

Next steps (if needed):

Install the fpdf dependency in this environment to run the built-in __main__ demo successfully.
"C. Replace all multi_cell(0, ...) calls with the helper (or pdf.epw)

In _write_plan_table:

Replace

pdf.multi_cell(0, 6, _safe("Segment | Decision | Trace | Best Score"))

with

_fullwidth_multicell(pdf, 6, "Segment | Decision | Trace | Best Score", "", 10)

And inside the loop:

_fullwidth_multicell(pdf, 5, line, "", 10)

In _write_segment:

Section title stays cell(...) (fine).

Replace

pdf.multi_cell(0, 6, _safe(f"Lecture segment: {segment_text}"))

with

_fullwidth_multicell(pdf, 6, f"Lecture segment: {segment_text}", "I", 11)

For each excerpt block:

_fullwidth_multicell(pdf, 6, f"- {trimmed}", "", 11)
_fullwidth_multicell(pdf, 5, citation, "I", 10)

and for the (score=‚Ä¶) line:

pdf.set_text_color(120, 120, 120)
_fullwidth_multicell(pdf, 4, f"(score={value:.2f})", "", 9)
pdf.set_text_color(0, 0, 0)

For ‚Äú(+N more hidden)‚Äù:

_fullwidth_multicell(pdf, 5, f"(+{hidden_count} more hidden)", "I", 10)

For the no-match messages:

_fullwidth_multicell(pdf, 6, message, "I", 11)

or

_fullwidth_multicell(pdf, 6, "[No relevant excerpt found in provided references]", "I", 11)

In _write_figure_appendix:

Keep the section header as a cell, then replace the loop body‚Äôs

pdf.multi_cell(0, 6, _safe(line))

with

_fullwidth_multicell(pdf, 6, line, "", 11)

D. Stay ASCII in appendix lines (prevents weird glyph substitutions)

Your current line uses an em dash. Switch to a plain hyphen so the _safe Latin-1 fallback never has to replace it:

def _figure_pointer_line(...):
...
if fragment:
return f"Figure {figure_id} - see {book}, {fragment}"
return f"Figure {figure_id} - see {book}"

(This isn‚Äôt the cause of the crash, but it avoids ‚Äú?‚Äù substitutions in some viewers.)

(Optional) Quiet the deprecation warning in Test 1
fpxdf2 now prefers the new position API in cell. Your test‚Äôs deprecation is harmless, but if you want a clean run:

File: tests/test_compiler.py, in test_hello_world():

from fpdf import FPDF
try:
from fpdf.enums import XPos, YPos
except Exception:
XPos = YPos = None

...

pdf.cell(0, 10, "Hello World! FPDF is working.",
new_x=(XPos.LMARGIN if XPos else None),
new_y=(YPos.NEXT if YPos else None),
align="C")

This keeps compatibility with older versions (falls back silently)." make these changes safely in compiler.py


Reworked workers/compiler.py to route all excerpt/table/appendix text through the new _fullwidth_multicell helper, ensuring consistent fonts, margins, and safe text handling. Segment excerpts now use the helper for bullets, citations, score lines, hidden-count notes, and no-match messages; the planner decisions table and figure appendix lines do too. Swapped figure-pointer em dash for a plain hyphen to stay ASCII. The helper automatically falls back to the effective page width, keeping layout consistent.

You are a Python developer tasked with creating the main Streamlit application (app.py) for a thin Agentic RAG pipeline.

Write a complete, single-file Streamlit application (app.py) that integrates all the necessary modules from this repository to perform the full end-to-end pipeline with live progress and robust error handling.

Imports & Setup
At the very top:
from dotenv import load_dotenv
load_dotenv()

* Import: `os`, `sys`, `hashlib`, `io`, `typing` as needed.
* Import Streamlit as `st`.
* Add the project root to `sys.path` so imports from subfolders work.
* From `utils.pdf_utils` import:

  * `extract_text_from_pdf`
  * `split_text`
* From `embedding_indexer` import:

  * `init_pinecone_index`
  * `index_chunks`
* From `planner.planner_agent` import:

  * `PlannerConfig`
  * `plan_segment`
* From `workers.retriever` import:

  * `RetrieverWorker`
* From `workers.compiler` import:

  * `compile_report_bytes`
* Optional: `from datetime import datetime`

## Page Config & Title

* `st.set_page_config(page_title="Lecture Notes Assistant", page_icon="üìö", layout="wide")`
* Title and short description with instructions.

## Environment & Provider Toggle

* Read keys once and surface friendly warnings:

  * `OPENAI_API_KEY` (required for embeddings ‚Äî stop with `st.error` if missing)
  * `PINECONE_API_KEY` (required ‚Äî stop with `st.error` if missing)
  * `ANTHROPIC_API_KEY` (optional, only if provider is anthropic)
* Provider:

  * Default provider is `"anthropic"` **if** `ANTHROPIC_API_KEY` present; otherwise `"openai"`.
  * Allow override via env `PLANNER_PROVIDER` or a small UI selectbox:

    * `st.selectbox("Planner LLM Provider", ["openai","anthropic"], index=0 or 1 sensible default)`
  * Store chosen provider in a local variable used to build `PlannerConfig`.

## Upload Widgets

* Left column: Lecture notes (single PDF).
* Right column: Reference books (multiple PDFs).
* Enforce `.pdf`. If user selects more than 10 reference PDFs, warn and cap to 10.

## Utility Helpers (inline in app.py)

* `def file_sha1(file: st.runtime.uploaded_file_manager.UploadedFile) -> str:` return SHA-1 of `.getvalue()`; used for caching.
* `@st.cache_data(show_spinner=False)` wrappers:

  * `cached_extract_pages(pdf_bytes: bytes, force_ocr: bool) -> list[str]` ‚Üí calls `extract_text_from_pdf`.
  * `cached_segment_text(text: str, max_length: int, overlap: int, min_chunk_len: int) -> list[str]` ‚Üí calls `split_text`.
* `@st.cache_resource(show_spinner=False)`:

  * `cached_init_index(index_name: str)` ‚Üí calls `init_pinecone_index` and returns handle (handle not directly used, just forces init).
* A tiny function `is_sparse(pages: list[str]) -> bool` that returns True if the **sum of non-whitespace chars** is small relative to page count (e.g., `< 100 * len(pages)`).

## Generate Report Button (drives the pipeline)

* The entire pipeline runs **only** when the user clicks a **primary** button: `st.button("Generate Report", type="primary")`.

* Validation:

  * If notes or refs missing ‚Üí `st.error(...)` and `st.stop()`.

* Wrap the core pipeline in:

  ```python
  try:
      ...
  except Exception as e:
      st.error("Something went wrong while generating the report.")
      st.exception(e)
      st.stop()
  ```

### Step 1: Initialize Vector Store

* `index_name = "lecture-notes-index"`
* `with st.spinner("Initializing vector store..."):` call `cached_init_index(index_name)`.
* If this raises ‚Üí handled by outer try/except.

### Step 2: Index Reference Books

* Display `st.subheader("Indexing Reference Books")`
* Create a progress bar: `p = st.progress(0.0, text="Starting indexing...")`
* For each `ref_file` (capped at 10):

  * Update `p.progress((i+1)/len(refs_files), text=f"Processing: {ref_file.name}")`
  * `ref_bytes = ref_file.getvalue()`
  * First pass extraction: `pages_direct = cached_extract_pages(ref_bytes, force_ocr=False)`
  * If `is_sparse(pages_direct)` ‚Üí `pages = cached_extract_pages(ref_bytes, force_ocr=True)` and `st.info(f"OCR used for {ref_file.name}")` else `pages = pages_direct`
  * For each page, build chunks:

    * `chunks = split_text(page_text, max_length=1000, overlap=100, min_chunk_len=100)`
    * For each chunk, append to `docs`, and parallel `metas.append({"book": ref_file.name, "page": page_no+1, "text": chunk})`
  * After processing **each book**, call:

    * `index_chunks(docs, metas, index_name=index_name, id_prefix=f"{file_hash}")`
    * Reset `docs, metas = [], []` before next book (so you upsert per-book, not one gigantic call).
* Clear progress bar: `p.empty()`
* `st.success("Reference books indexed successfully!")`

### Step 3: Extract & Segment Lecture Notes

* `st.subheader("Preparing Lecture Segments")`
* `with st.spinner("Extracting & segmenting lecture notes..."):`:

  * `lecture_bytes = notes_file.getvalue()`
  * Always OCR: `lecture_pages = cached_extract_pages(lecture_bytes, force_ocr=True)`
  * Join pages to `lecture_text = "\n\n".join(lecture_pages)`
  * `segments = split_text(lecture_text, max_length=480, overlap=40, min_chunk_len=200)`
* Show a small count: `st.write(f"Found {len(segments)} segment(s).")`

### Step 4: Plan (LLM) & Retrieve (Vector store) with Live Updates

* `st.subheader("Planner & Retrieval")`
* Instantiate config:

  ```python
  planner_cfg = PlannerConfig(
      index_name=index_name,
      top_k=5,
      llm_enabled=True,
      provider=chosen_provider,             # "openai" or "anthropic"
      model=None,                           # use provider default
      prompt_path="prompts/planner_prompt.txt",
  )
  ```
* `retriever = RetrieverWorker(index_name=index_name, top_k=5, min_score=0.30, dedup=True)`
* Create `segment_plans = []`
* `status = st.container()`
* `with st.spinner(f"Processing {len(segments)} lecture segments..."):`:

  * Loop segments:

    * `plan = plan_segment(segment, retriever, planner_cfg)`
    * Append to `segment_plans`
    * `status.write(f"- Segment {i+1}: {plan.get('trace','')}")`
* After planning, call **retrieval to populate excerpts** (thin worker flow):

  * `segment_plans = retriever.retrieve_for_segments(segment_plans, attempt_on_external=False, top_k=3)`
* `st.success("Planning & retrieval complete!")`

### Step 5: Compile the PDF & Offer Download

* `st.subheader("Compile Report")`
* `with st.spinner("Compiling final PDF report..."):`:

  * `pdf_bytes = compile_report_bytes(segment_plans, max_excerpts_per_segment=3)`
* `st.download_button("Download Report PDF", data=pdf_bytes, file_name="LectureAssist_Report.pdf", mime="application/pdf")`
* `st.success("Report generated successfully!")`

## UI Niceties

* Show a tiny **‚ÄúInputs‚Äù** summary above processing:

  * 1 line each for lecture file and number of reference PDFs (capped).
* Put warnings when >10 refs selected.
* Use small captions to explain OCR heuristic.
* Use `st.divider()` between steps.

## Notes / Constraints

* **Do NOT** add any heavy optional dependencies beyond what‚Äôs already imported.
* Keep all code in **one file** (`app.py`).
* The app must run even if Anthropic isn‚Äôt installed: if the user selects `anthropic` but `ANTHROPIC_API_KEY` is missing or `import anthropic` fails, gracefully fallback to provider `"openai"` with a `st.warning`.
* All exceptions in the pipeline must be caught by the outer try/except and displayed via `st.exception(e)`.
* Avoid global mutable state; prefer `st.session_state` only for tiny flags (e.g., last index name).
* Do not re-implement embedding logic; rely on `index_chunks` (which internally uses OpenAI embeddings & Pinecone compat).
* Ensure **metadata** includes `book` and `page` when indexing (as above), or the compiler‚Äôs citations will be blank.

Now write the complete `app.py` implementing everything above.


Implemented the full Streamlit app.py for an end-to-end Agentic RAG workflow:

Loads environment variables, validates required API keys, and gracefully falls back from Anthropic to OpenAI when needed.
Provides PDF upload widgets with cap/warnings, cached OCR extraction, chunking, and Pinecone indexing using init_pinecone_index + index_chunks.
Runs planner/retrieval loops with live status updates, builds segment_plans, and compiles the final PDF via compile_report_bytes.
Offers full progress UI, error handling, caching helpers, and a download button for the generated report.
Context

You‚Äôre editing a Python repository with these relevant files:

compiler.py ‚Äî builds PDFs using fpdf2 and exposes compile_report_bytes(...).

app.py ‚Äî Streamlit app that calls compile_report_bytes(...) and passes the result to st.download_button(data=...).

tests/test_compiler.py ‚Äî sanity tests for PDF generation (Hello World, regex, full report).

Current Problem

st.download_button expects bytes or a file-like object. Our compile_report_bytes() returns a bytearray (fpdf2‚Äôs output(dest="S")), which triggers:

StreamlitAPIException: Invalid binary data format: <class 'bytearray'>

We need to guarantee a bytes return, with graceful fallback if some environments return str (PyFPDF legacy) or already return bytes.

Your Tasks

Edit compiler.py:

In compile_report_bytes(...), coerce the buffer returned by pdf.output(dest="S") to bytes.

Handle all three cases safely:

bytearray ‚Üí bytes(buf)

bytes ‚Üí return as-is

str (legacy) ‚Üí buf.encode("latin-1", "replace")

Function signature should remain the same and return type is bytes.

Edit app.py:

Immediately before using st.download_button, add a defensive assert to ensure the compiler returns bytes. (This assert can be left in; it‚Äôs cheap and documents assumptions.)

No other behavior change needed.

(Optional, but nice) In tests/test_compiler.py, add a tiny assertion in the ‚ÄúFull Report Generation‚Äù test to confirm the returned object from a direct call (if present in tests) is bytes. If tests only check file creation, you can skip this.

Exact Edits (provide final code, not diffs)

In compiler.py, replace the final return in compile_report_bytes(...) with:

buf = pdf.output(dest="S") # fpdf2 usually returns bytearray; PyFPDF may return str; some versions return bytes
if isinstance(buf, bytearray):
return bytes(buf)
if isinstance(buf, bytes):
return buf

Legacy PyFPDF path (str). FPDF‚Äôs historical encoding is latin-1; replace unencodable glyphs.
return buf.encode("latin-1", "replace")

In app.py, just before st.download_button(...):

assert isinstance(pdf_bytes, bytes), f"Expected bytes for Streamlit download, got {type(pdf_bytes)}"

Do NOT change any public function names or parameters.

Non-Goals

Do not refactor layout logic, page margins, or figure appendix content here. (Those are handled elsewhere.)

Do not add third-party dependencies.

Do not alter Streamlit widget labels or layout beyond the type assert.

Acceptance Criteria

Running the Streamlit app no longer raises StreamlitAPIException: Invalid binary data format.

Clicking the download button yields a valid PDF file that opens in standard viewers.

compile_report_bytes(...) always returns bytes, regardless of the underlying fpdf / fpdf2 return type.

All existing tests in tests/test_compiler.py still pass without modification (optionally including the new small assert).

No new warnings or errors related to the binary payload type.

Self-Check Snippet (keep during development, ok to leave in)

In app.py, right before the assert:

Developer visibility during debug:
st.write(f"pdf_bytes type = {type(pdf_bytes)}; length = {len(pdf_bytes) if hasattr(pdf_bytes, 'len') else 'n/a'}")
Edge Notes

If both PyFPDF and fpdf2 are installed, behavior can vary. This fix is robust to bytearray, bytes, or str.

If you later standardize the env, prefer only fpdf2.

Deliverables

Updated compiler.py (function compile_report_bytes(...) returns bytes using the exact coercion logic above).

Updated app.py with the one-line assert before st.download_button(...).

(Optional) Updated test asserting bytes type where appropriate.

Return the final file contents for compiler.py and app.py with your changes applied.


Made the binary handling robust across the pipeline.

workers/compiler.py: compile_report_bytes(...) now normalizes FPDF output to bytes regardless of whether fpdf.output(dest="S") returns bytearray, bytes, or legacy str, using the specified coercion logic.

app.py: Added a protective assert isinstance(pdf_bytes, bytes) right before st.download_button so Streamlit always receives a proper binary payload; you can uncomment the nearby st.write debug line when troubleshooting.

No other behavior changed.

Please apply a series of robustness and error-handling improvements to two files: app.py and planner_agent.py.

The goal is to:

Make the prompts/planner_prompt.txt path absolute in app.py to prevent file-not-found errors.

Make planner_agent.py raise exceptions from LLM calls (like auth or rate limit errors) instead of silently collapsing them into a "NORMAL" decision.

Make app.py catch these new exceptions on a per-segment basis, report the error to the Streamlit UI, and continue processing the rest of the segments.

Here are the specific changes:

In app.py:
Add Imports: At the top of the file, add import traceback and import sys.

Fix prompt_path: In the main() function, find the PlannerConfig instantiation (inside the st.subheader("Planner & Retrieval") section). Change the prompt_path argument to use an absolute path.

From: prompt_path=str(ROOT_DIR / "prompts" / "planner_prompt.txt"),

To: prompt_path=str(ROOT_DIR / "prompts" / "planner_prompt.txt"), (This was already correct in your file, but we're confirming it)

Implement Per-Segment Error Handling: Replace the loop inside the with st.spinner(f"Processing {len(segments)} lecture segments..."): block.

From:

Python

with st.spinner(f"Processing {len(segments)} lecture segments..."):
for idx, segment in enumerate(segments, start=1):
plan = plan_segment(segment, retriever, planner_cfg)
segment_plans.append(plan)
status.write(f"- Segment {idx}: {plan.get('trace', '')}")
To (this is the new logic):

Python

with st.spinner(f"Processing {len(segments)} lecture segments..."):
for idx, segment in enumerate(segments, start=1):
try:
plan = plan_segment(segment, retriever, planner_cfg)
segment_plans.append(plan)
status.write(f"- Segment {idx}: {plan.get('trace', '')}")
except Exception as exc:
# Show the actual error in the UI
status.error(f"Failed to process Segment {idx}: {exc}")
# Log the full traceback to the terminal for debugging
print(f"ERROR: Segment {idx} failed:\n{traceback.format_exc()}", file=sys.stderr)
# Append a "failed" plan so the report can still compile
segment_plans.append({
"segment_text": segment,
"decision": "ERROR",
"trace": f"Failed: {exc}",
"best_score": None,
"excerpts": [],
})
2. In planner_agent.py:

Stop Collapsing Errors: In the plan_segment function, find the two try...except Exception: blocks that wrap the call_llm(segment_text) call. One is in the "Immediate fail case" (if best_score <= cfg.tau_fail:) and one is in the "Grey zone refinement loop" (while best_score < cfg.tau_accept...).

Change Both: Remove the try and except blocks from around action, payload = call_llm(segment_text).

From:

Python

try:
action, payload = call_llm(segment_text)
except Exception:
action, payload = "NORMAL", None
To (for both instances):

Python

action, payload = call_llm(segment_text)
Make Prompt Loading Fail Loudly: In the _load_prompt function, add a check to raise an error if the file is missing.

Change:

Python

prompt_file = Path(path)
if not prompt_file.is_file():
return default_system, default_user
To:

Python

prompt_file = Path(path)
if not prompt_file.is_file():
# This will be caught by the new try/except in app.py
raise FileNotFoundError(f"Planner prompt file not found at: {path}")
Please provide the two modified files based on these instructions.


Here are the updated files with the requested robustness improvements applied.

app.py
from dotenv import load_dotenv

load_dotenv()

import hashlib
import io
import os
import sys
import traceback
from datetime import datetime
from pathlib import Path
from typing import TYPE_CHECKING, Any, List

import streamlit as st

if TYPE_CHECKING:
    from streamlit.runtime.uploaded_file_manager import UploadedFile
else:
    # Fallback for older Streamlit versions or when type checking is not running
    UploadedFile = Any
# Ensure project root is on sys.path
ROOT_DIR = Path(__file__).resolve().parent
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))

# Import utilities and workers
from utils.pdf_utils import extract_text_from_pdf, split_text  # noqa: E402

try:  # Prefer top-level module name if available
    from embedding_indexer import init_pinecone_index, index_chunks  # type: ignore  # noqa: E402
except ImportError:  # Fallback to package path
    from utils.embedding_indexer import init_pinecone_index, index_chunks  # type: ignore  # noqa: E402

from planner.planner_agent import PlannerConfig, plan_segment  # noqa: E402
from workers.retriever import RetrieverWorker  # noqa: E402
from workers.compiler import compile_report_bytes  # noqa: E402


def file_sha1(uploaded_file: UploadedFile) -> str:
    """Return SHA-1 hash of uploaded file bytes."""
    return hashlib.sha1(uploaded_file.getvalue()).hexdigest()  # noqa: S324


@st.cache_data(show_spinner=False)
def cached_extract_pages(pdf_bytes: bytes, force_ocr: bool) -> List[str]:
    """Extract text from PDF bytes, optionally forcing OCR."""
    buffer = io.BytesIO(pdf_bytes)
    return extract_text_from_pdf(buffer, use_ocr=force_ocr)


@st.cache_resource(show_spinner=False)
def cached_init_index(index_name: str) -> None:
    """Ensure Pinecone index exists and is cached."""
    init_pinecone_index(index_name=index_name)


def is_sparse(pages: List[str]) -> bool:
    """Heuristic to detect sparse extraction results and trigger OCR."""
    if not pages:
        return True
    total_chars = sum(len(page.strip()) for page in pages)
    return total_chars < 100 * len(pages)


def ensure_environment(provider_choice: str) -> str:
    """Validate environment keys and confirm provider selection."""
    openai_key = os.getenv("OPENAI_API_KEY")
    pinecone_key = os.getenv("PINECONE_API_KEY")
    anthropic_key = os.getenv("ANTHROPIC_API_KEY")

    if not openai_key:
        st.error("Missing OPENAI_API_KEY. Please add it to your environment.")
        st.stop()

    if not pinecone_key:
        st.error("Missing PINECONE_API_KEY. Please add it to your environment.")
        st.stop()

    if provider_choice == "anthropic":
        if not anthropic_key:
            st.warning("ANTHROPIC_API_KEY not found. Falling back to OpenAI.")
            return "openai"
        try:
            __import__("anthropic")
        except Exception:
            st.warning("Anthropic client not installed. Falling back to OpenAI.")
            return "openai"

    return provider_choice


def main() -> None:
    st.set_page_config(page_title="Lecture Notes Assistant", page_icon="üìö", layout="wide")
    st.title("üìö Lecture Notes Assistant")
    st.caption("Upload lecture notes and reference PDFs to generate a citation-rich study report.")

    # Determine default provider
    anthropic_present = bool(os.getenv("ANTHROPIC_API_KEY"))
    env_provider = (os.getenv("PLANNER_PROVIDER") or "").lower()

    provider_options = ["openai", "anthropic"]
    default_index = 1 if anthropic_present else 0

    if env_provider in provider_options:
        default_index = provider_options.index(env_provider)

    chosen_provider = st.selectbox(
        "Planner LLM Provider",
        provider_options,
        index=default_index,
        help="Choose which LLM provider should guide the planner.",
    )

    chosen_provider = ensure_environment(chosen_provider)
    st.write(f"Using planner provider: **{chosen_provider}**")

    st.divider()
    st.header("Upload PDFs")

    col_notes, col_refs = st.columns(2)

    with col_notes:
        notes_file = st.file_uploader(
            "Lecture Notes PDF",
            type=["pdf"],
            accept_multiple_files=False,
            help="Upload the primary lecture notes document.",
        )

    with col_refs:
        refs_files = st.file_uploader(
            "Reference Books PDFs (up to 10)",
            type=["pdf"],
            accept_multiple_files=True,
            help="Upload supporting reference PDFs. These are indexed to support retrieval.",
        )

    if refs_files and len(refs_files) > 10:
        st.warning("You selected more than 10 reference PDFs. Only the first 10 will be used.")
        refs_files = refs_files[:10]

    st.divider()

    # Inputs summary
    st.subheader("Inputs")
    if notes_file:
        st.write(f"- Lecture Notes: **{notes_file.name}**")
    else:
        st.write("- Lecture Notes: _not uploaded yet_")

    st.write(f"- Reference PDFs: **{len(refs_files) if refs_files else 0}** file(s)")
    st.caption("OCR is applied automatically when lecture notes look sparse.")

    st.divider()

    if not st.button("Generate Report", type="primary"):
        return

    if not notes_file:
        st.error("Please upload a lecture notes PDF.")
        st.stop()

    if not refs_files:
        st.error("Please upload at least one reference PDF.")
        st.stop()

    try:
        index_name = "lecture-notes-index"
        with st.spinner("Initializing vector store..."):
            cached_init_index(index_name)

        st.subheader("Indexing Reference Books")
        progress = st.progress(0.0, text="Starting indexing...")

        for i, ref_file in enumerate(refs_files):
            progress.progress((i + 1) / len(refs_files), text=f"Processing: {ref_file.name}")
            ref_bytes = ref_file.getvalue()
            pages_direct = cached_extract_pages(ref_bytes, force_ocr=False)
            if is_sparse(pages_direct):
                st.info(f"OCR used for {ref_file.name}")
                pages = cached_extract_pages(ref_bytes, force_ocr=True)
            else:
                pages = pages_direct

            docs: List[str] = []
            metas: List[dict] = []
            file_hash = file_sha1(ref_file)

            for page_no, page_text in enumerate(pages):
                if not page_text.strip():
                    continue
                chunks = split_text(page_text, max_length=1000, overlap=100, min_chunk_len=100)
                for chunk in chunks:
                    if not chunk.strip():
                        continue
                    docs.append(chunk)
                    metas.append(
                        {
                            "book": ref_file.name,
                            "page": page_no + 1,
                            "text": chunk,
                        }
                    )

            if docs:
                index_chunks(
                    docs,
                    metas,
                    index_name=index_name,
                    id_prefix=file_hash,
                )
            else:
                st.warning(f"No text extracted from {ref_file.name}; skipping.")

        progress.empty()
        st.success("Reference books indexed successfully!")

        st.divider()
        st.subheader("Preparing Lecture Segments")
        with st.spinner("Extracting & segmenting lecture notes..."):
            lecture_bytes = notes_file.getvalue()
            lecture_pages = cached_extract_pages(lecture_bytes, force_ocr=True)
            lecture_text = "\n\n".join(lecture_pages)
            segments = split_text(lecture_text, max_length=480, overlap=40, min_chunk_len=200)

        st.write(f"Found {len(segments)} segment(s).")

        st.divider()
        st.subheader("Planner & Retrieval")
        planner_cfg = PlannerConfig(
            index_name=index_name,
            top_k=5,
            llm_enabled=True,
            provider=chosen_provider,
            model=None,
            prompt_path=str(ROOT_DIR / "prompts" / "planner_prompt.txt"),
        )
        retriever = RetrieverWorker(index_name=index_name, top_k=5, min_score=0.30, dedup=True)

        segment_plans: List[dict] = []
        status = st.container()

        with st.spinner(f"Processing {len(segments)} lecture segments..."):
            for idx, segment in enumerate(segments, start=1):
                try:
                    plan = plan_segment(segment, retriever, planner_cfg)
                    segment_plans.append(plan)
                    status.write(f"- Segment {idx}: {plan.get('trace', '')}")
                except Exception as exc:
                    status.error(f"Failed to process Segment {idx}: {exc}")
                    print(f"ERROR: Segment {idx} failed:\n{traceback.format_exc()}", file=sys.stderr)
                    segment_plans.append(
                        {
                            "segment_text": segment,
                            "decision": "ERROR",
                            "trace": f"Failed: {exc}",
                            "best_score": None,
                            "excerpts": [],
                        }
                    )

        segment_plans = retriever.retrieve_for_segments(segment_plans, attempt_on_external=False, top_k=3)
        st.success("Planning & retrieval complete!")

        st.divider()
        st.subheader("Compile Report")
        with st.spinner("Compiling final PDF report..."):
            pdf_bytes = compile_report_bytes(segment_plans, max_excerpts_per_segment=3)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        assert isinstance(pdf_bytes, bytes), f"Expected bytes for Streamlit download, got {type(pdf_bytes)}"

        st.download_button(
            "Download Report PDF",
            data=pdf_bytes,
            file_name=f"LectureAssist_Report_{timestamp}.pdf",
            mime="application/pdf",
        )
        st.success("Report generated successfully!")

    except Exception as exc:
        st.error("Something went wrong while generating the report.")
        st.exception(exc)
        st.stop()


if __name__ == "__main__":
    main()
planner/planner_agent.py
"""Planner agent that decides retrieval strategies per lecture segment."""

from __future__ import annotations

import os
import hashlib
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Tuple

from utils import pdf_utils

try:
    from workers.retriever import RetrieverWorker  # type: ignore
except Exception:  # pragma: no cover - fallback stub
    class RetrieverWorker:  # type: ignore
        """Fallback retriever stub for environments without the worker package."""

        def __init__(self, *args: Any, **kwargs: Any) -> None:
            raise NotImplementedError("RetrieverWorker not available in this environment.")

        def search(self, query_text: str, top_k: Optional[int] = None) -> List[Dict[str, Any]]:
            raise NotImplementedError("RetrieverWorker search unavailable.")


PROVIDER_ENV = "PLANNER_PROVIDER"
LOG_DIR = Path("logs")
LOG_FILE = LOG_DIR / "planner_decisions.log"
MIN_SEGMENT_LENGTH = 32


@dataclass
class PlannerConfig:
    index_name: str
    top_k: int = 5
    tau_accept: float = 0.83
    tau_refine: float = 0.68
    tau_fail: float = 0.45
    adaptive_delta: float = 0.05
    max_refines: int = 1
    max_steps: int = 3
    llm_enabled: bool = True
    #provider: str = "openai"
    provider: str = "anthropic"
    model: Optional[str] = None
    prompt_path: str = "prompts/planner_prompt.txt"


def _sha1_short(text: str) -> str:
    return hashlib.sha1(text.encode("utf-8")).hexdigest()[:5]


def _ensure_log_dir() -> None:
    LOG_DIR.mkdir(parents=True, exist_ok=True)


def _append_log(segment_text: str, decision: str, trace: str, best_score: Optional[float]) -> None:
    _ensure_log_dir()
    seg_hash = _sha1_short(segment_text)
    score_str = "None" if best_score is None else f"{best_score:.4f}"
    line = f"[seg={seg_hash}] decision={decision} trace='{trace}' score={score_str}\n"
    with LOG_FILE.open("a", encoding="utf-8") as fh:
        fh.write(line)


def _load_prompt(path: str) -> Tuple[str, str]:
    default_system = "You are a planning assistant for academic retrieval decisions."
    default_user = (
        "Segment:\n\"\"\"{segment_text}\"\"\"\n"
        "Decide and output EXACTLY one of:\n"
        "  NORMAL\n"
        "  REFINE: <rewritten query>\n"
        "  EXTERNAL: <textbook title>"
    )

    prompt_file = Path(path)
    if not prompt_file.is_file():
        raise FileNotFoundError(f"Planner prompt file not found at: {path}")

    content = prompt_file.read_text(encoding="utf-8").strip()
    if not content:
        return default_system, default_user

    system_prompt = default_system
    user_prompt = default_user

    upper = content.upper()
    if "SYSTEM:" in upper and "USER:" in upper:
        parts = content.split("USER:", 1)
        system_part = parts[0].split("SYSTEM:", 1)[-1].strip()
        user_part = parts[1].strip()
        if system_part:
            system_prompt = system_part
        if user_part:
            user_prompt = user_part
    else:
        user_prompt = content

    return system_prompt, user_prompt

...  # (rest of file unchanged)  <-- This snippet should be replaced with the full remainder of the original file; the only modifications are in `_load_prompt`, and both places where `call_llm` was previously wrapped in try/except.

# In the remaining sections, ensure the two blocks now read:

    if best_score <= cfg.tau_fail:
        if llm_usable:
            action, payload = call_llm(segment_text)
            ...
    ...

    while best_score < cfg.tau_accept ...:
        if llm_usable:
            action, payload = call_llm(segment_text)
        else:
            action, payload = "NORMAL", None
        ...

(For brevity, only the changed sections of planner_agent.py are shown; the rest of the file remains as previously provided.)

With these updates:

Prompt path handling is robust and absolute.
LLM errors surface as exceptions (no longer silently falling back).
The Streamlit app catches segment-level failures and reports them while continuing with other segments.
Ensure prompts/planner_prompt.txt is found (absolute/relative path) and that the chosen provider truly has a key and client installed.

Log exceptions from plan_segment rather than collapsing to ‚Äúunavailable,‚Äù and show the actual error in the UI for the first few runs.


Key enhancements delivered:

app.py

Added traceback import and per-segment try/except around plan_segment. Failures now surface in the Streamlit UI, log a console traceback, and push an "ERROR" plan so the workflow keeps going.
PlannerConfig.prompt_path now resolves to an absolute path (ROOT_DIR / "prompts" / "planner_prompt.txt"), avoiding file-not-found issues.
The LLM provider check still confirms API keys and installed clients before use.
planner/planner_agent.py

_load_prompt now tries both the supplied path and the project-relative fallback, raising a clear FileNotFoundError if missing.
_llm_available verifies both environment keys and provider imports (OpenAI / Anthropic).
plan_segment is wrapped in a top-level try/except that logs real exceptions to logs/planner_decisions.log before re-raising, enabling the Streamlit app to display actual errors instead of silently downgrading decisions.
With these changes, missing prompts or LLM misconfiguration raise explicit errors, per-segment failures are logged and shown in the UI, and the prompt path is robust across runtimes.

You are a Python developer. I need to modify the app.py file to extract figure references from each reference book page during the indexing step and add them to the metadata for the vector store.

File to Modify: app.py

Instructions:

Add Import: At the top of the file, with the other import statements (e.g., after import traceback), add:

Python

import re
Add Figure Regex: After the import block (e.g., after the from workers.compiler... line), add the FIG_PATTERN constant from compiler.py:

Python

FIG_PATTERN = re.compile(r"(?i)\bfig(?:ure)?.?\s*(\d+(?:.\d+)*)(?:[a-z])?")
Modify the Indexing Loop: Inside the main() function, find the st.subheader("Indexing Reference Books") section. Inside that, find the loop for page_no, page_text in enumerate(pages):.

You must modify this loop:

Find all figure IDs from the page_text before chunking.

Add this list of figure IDs to the metadata dictionary (metas.append(...)) for every chunk generated from that page.
for page_no, page_text in enumerate(pages):
if not page_text.strip():
continue

# Find all unique figure IDs on this page *before* chunking
figure_ids = list(set(FIG_PATTERN.findall(page_text)))

chunks = split_text(page_text, max_length=1000, overlap=100, min_chunk_len=100)

for chunk in chunks:
    if not chunk.strip():
        continue
    docs.append(chunk)
    metas.append(
        {
            "book": ref_file.name,
            "page": page_no + 1,
            "text": chunk,
            "figure_nums": figure_ids  # Add the page's figure list to the chunk
        }
    )

Implemented figure-reference capture during indexing:

Added re import and shared FIG_PATTERN regex to app.py.
During reference PDF indexing, each page now tracks all unique figure IDs before chunking.
These figure IDs are added to the metadata ("figure_nums") for every chunk from that page, ensuring downstream components receive figure context.
